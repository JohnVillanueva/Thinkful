{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('../__DATA__/twitter-sentiment-analysis2/train.csv', encoding = 'ISO-8859-1' )\n",
    "test_set = pd.read_csv('../__DATA__/twitter-sentiment-analysis2/test.csv', encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99989, 4)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299989, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         is so sad for my APL frie...\n",
       "1                       I missed the New Moon trail...\n",
       "2                              omg its already 7:30 :O\n",
       "3              .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4             i think mi bf is cheating on me!!!   ...\n",
       "Name: SentimentText, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.SentimentText[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         is so sad for my APL frie...\n",
       "1                       I missed the New Moon trail...\n",
       "2                              omg its already 7:30 :O\n",
       "3              .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4             i think mi bf is cheating on me!!!   ...\n",
       "Name: SentimentText, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.SentimentText[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(train_set.SentimentText, train_set.Sentiment,\n",
    "                                                    train_size = 0.5, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Bsimi thank you Brian',\n",
       " \"cavsfanatic Fantastic now I want chocolate milk And there's no Nesquik in the house\"]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "ndocs = 5000\n",
    "\n",
    "X_train_cleaned = []\n",
    "for i, doc in enumerate(X_train[:ndocs]):\n",
    "    doc = re.sub(r\"\\n\", \" \", doc) # new lines to spaces\n",
    "    doc = re.sub(\"[^a-zA-Z' ]+\", '', doc) # rid of punctuation and numbers\n",
    "    doc = re.sub(' +', ' ', doc) # stripping extra white space out\n",
    "    doc = doc.strip() # stripping extra white space out\n",
    "    X_train_cleaned.append(doc)\n",
    "\n",
    "print(len(X_train_cleaned))\n",
    "X_train_cleaned[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bsimi thank you Brian',\n",
       " \"cavsfanatic Fantastic now I want chocolate milk And there's no Nesquik in the house\",\n",
       " 'Suze I like the food angle to it Thanks',\n",
       " 'CrazyCatLadie and cutiepie sorry had prior plans for today',\n",
       " \"complianceweek CONGRATS I refollowed you several times as you got closer but guess I didn't win\",\n",
       " \"sigh ok where was I oh yesgoing to john's doughnut for coffee n light reading dty cleaners then the gymowww a sale\",\n",
       " \"AndreaDG oooh Marty's cracklins Yumo Haha I love that\",\n",
       " 'shoutout my mommy shes sick and it worries me please keep her in ur prayers',\n",
       " 'bombDUH ugghhhh the worst',\n",
       " \"TraceyMmm hahaha it's my idea my hashtag i stole it from a pub in Fortitude Valley NoUndiesSunday\"]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cleaned[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iremember how i was stunned by that txt msg gt still am sometimes ohhhh yea httptinyurlcomlmovyf',\n",
       " \"NO MORE SICKNESS strike i say STRIKE ohhhh well 'nother day off school i guess\",\n",
       " \"ambermac well it's got to be better than twitterberry\",\n",
       " 'comefilljulia I feel some love But I have to go to work in a few minutes',\n",
       " 'ConnieLeyva lol i dnt thnk so but el taquito sounds yum',\n",
       " 'bubblythoughts Yup very happy But still have a thesis to finish writing SO excited to start working hows your morning going',\n",
       " 'ClareBear I wonder if eckstavo will pee his pants in excitement Wanna take bets',\n",
       " \"atchoo you go my girl that's the foremost reason why i love you re me being me\",\n",
       " \"caitlinlavergne they are going to destin I want to go to the beach so badbut I'd rather work\",\n",
       " 'none of the girls I know at Halmark are working My poor lunch will be warm by the time I eat it its gonna be warm']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cleaned[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bsimi thank Brian',\n",
       " 'cavsfanatic Fantastic want chocolate milk Nesquik house']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "tokenized_train_docs = []\n",
    "documents = [ nlp(document) for document in X_train_cleaned ]\n",
    "\n",
    "for text in documents:\n",
    "    tokens = [token.lemma_\n",
    "                    for token in text\n",
    "                    if not token.is_punct\n",
    "                    and not token.is_stop]\n",
    "    doc_parsed = ' '.join(tokens)\n",
    "    tokenized_train_docs.append(doc_parsed)\n",
    "\n",
    "tokenized_train_docs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=True, \n",
    "                          stop_words='english',\n",
    "                          ngram_range=(1, 1), \n",
    "                          analyzer=u'word', \n",
    "                          max_df=.5, \n",
    "                          min_df=1,\n",
    "                          max_features=None, \n",
    "                          vocabulary=None, \n",
    "                          binary=False)\n",
    "\n",
    "train_tfidf = vectorizer.fit_transform(tokenized_train_docs)\n",
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "X_test_cleaned = []\n",
    "for i, doc in enumerate(X_test[:ndocs]):\n",
    "    doc = re.sub(r\"\\n\", \" \", doc) # new lines to spaces\n",
    "    doc = re.sub(\"[^a-zA-Z' ]+\", '', doc) # rid of punctuation and numbers\n",
    "    doc = re.sub(' +', ' ', doc) # stripping extra white space out\n",
    "    doc = doc.strip() # stripping extra white space out\n",
    "    X_test_cleaned.append(doc) \n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "tokenized_test_docs = []\n",
    "documents = [ nlp(document) for document in X_test_cleaned ]\n",
    "\n",
    "for text in documents:\n",
    "    tokens = [token.lemma_\n",
    "                    for token in text\n",
    "                    if not token.is_punct\n",
    "                    and not token.is_stop]\n",
    "    doc_parsed = ' '.join(tokens)\n",
    "    tokenized_test_docs.append(doc_parsed)\n",
    "\n",
    "# tfidf transformation, based on train fitting\n",
    "test_tfidf = vectorizer.transform(tokenized_test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 11732)\n",
      "(5000, 11732)\n"
     ]
    }
   ],
   "source": [
    "print(train_tfidf.shape)\n",
    "print(test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up a Keras Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Import the backend\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 train samples\n",
      "5000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Initializing final train and test sets\n",
    "#testlen = 1000\n",
    "\n",
    "X_Train = train_tfidf\n",
    "Y_Train = keras.utils.to_categorical(Y_train[:ndocs], 2)\n",
    "X_Test = test_tfidf\n",
    "Y_Test = keras.utils.to_categorical(Y_test[:ndocs], 2)\n",
    "\n",
    "X_Train = X_Train.astype('float32')\n",
    "X_Test = X_Test.astype('float32')\n",
    "\n",
    "print(X_Train.shape[0], 'train samples')\n",
    "print(X_Test.shape[0], 'test samples')\n",
    "\n",
    "# In case you're confused about normalizing tfidf vectors, just know that tfidf matrices are meant to processed as is\n",
    "# No need for further normalization, because two normalizations are already baked into the term itself\n",
    "# See: https://datascience.stackexchange.com/questions/33730/should-i-rescale-tfidf-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11732"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = X_Train.shape[1]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 512)               6007296   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                16416     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 6,023,778\n",
      "Trainable params: 6,023,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, activation='relu', input_shape=(input_shape,)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "5000/5000 [==============================] - 2s 483us/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 2.6417 - val_accuracy: 0.6436\n",
      "Epoch 2/10\n",
      "5000/5000 [==============================] - 2s 481us/step - loss: 5.9406e-04 - accuracy: 0.9998 - val_loss: 2.7296 - val_accuracy: 0.6406\n",
      "Epoch 3/10\n",
      "5000/5000 [==============================] - 2s 479us/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 2.7438 - val_accuracy: 0.6448\n",
      "Epoch 4/10\n",
      "5000/5000 [==============================] - 2s 481us/step - loss: 8.6366e-04 - accuracy: 0.9996 - val_loss: 2.7952 - val_accuracy: 0.6496\n",
      "Epoch 5/10\n",
      "5000/5000 [==============================] - 2s 482us/step - loss: 9.1056e-04 - accuracy: 0.9996 - val_loss: 2.8358 - val_accuracy: 0.6466\n",
      "Epoch 6/10\n",
      "5000/5000 [==============================] - 2s 484us/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 2.8739 - val_accuracy: 0.6484\n",
      "Epoch 7/10\n",
      "5000/5000 [==============================] - 2s 485us/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 2.9289 - val_accuracy: 0.6434\n",
      "Epoch 8/10\n",
      "5000/5000 [==============================] - 2s 483us/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 2.9779 - val_accuracy: 0.6468\n",
      "Epoch 9/10\n",
      "5000/5000 [==============================] - 2s 483us/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 3.0170 - val_accuracy: 0.6484\n",
      "Epoch 10/10\n",
      "5000/5000 [==============================] - 2s 482us/step - loss: 6.5189e-04 - accuracy: 0.9998 - val_loss: 3.1445 - val_accuracy: 0.6418\n",
      "Test loss: 3.1444558094024657\n",
      "Test accuracy: 0.6417999863624573\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_Train, Y_Train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_Test, Y_Test))\n",
    "score = model.evaluate(X_Test, Y_Test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network\n",
    "\n",
    "We're starting from scratch! This time using GloVe word embeddings to vectorize the text and running it through a recurrent Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "#from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import os\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('../__DATA__/twitter-sentiment-analysis2/train.csv', encoding = 'ISO-8859-1' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['is so sad for my APL friend', 'I missed the New Moon trailer']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "ndocs = 10000\n",
    "\n",
    "docs_cleaned = []\n",
    "for i, doc in enumerate(train_set.SentimentText[:ndocs]):\n",
    "    doc = re.sub(r\"\\n\", \" \", doc) # new lines to spaces\n",
    "    doc = re.sub(\"[^a-zA-Z' ]+\", '', doc) # rid of punctuation and numbers\n",
    "    doc = re.sub(' +', ' ', doc) # stripping extra white space out\n",
    "    doc = doc.strip() # stripping extra white space out\n",
    "    docs_cleaned.append(doc)\n",
    "\n",
    "print(len(docs_cleaned))\n",
    "docs_cleaned[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predetermined Constants\n",
    "MAX_SEQUENCE_LENGTH = 117\n",
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Tokens 19620\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(docs_cleaned)\n",
    "sequences = tokenizer.texts_to_sequences(docs_cleaned)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Number of Unique Tokens',len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(seq) for seq in sequences]) # Setting MAX_NB_SEQUENCES thusly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Data Tensor: (10000, 117)\n",
      "Shape of Label Tensor: (10000, 2)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(train_set.Sentiment[:ndocs]))\n",
    "print('Shape of Data Tensor:', data.shape)\n",
    "print('Shape of Label Tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 1193514 word vectors in Glove 27B 100d.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('../__TRAINED_MODELS__/glove.twitter.27B/glove.twitter.27B.100d.txt',encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors in Glove 27B 100d.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bidirectional LSTM\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 117)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 117, 100)          1962100   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 200)               160800    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 2,123,302\n",
      "Trainable params: 2,123,302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_lstm = Bidirectional(LSTM(100))(embedded_sequences)\n",
    "preds = Dense(2, activation='softmax')(l_lstm)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"Bidirectional LSTM\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 276s 34ms/step - loss: 0.5228 - acc: 0.7475 - val_loss: 0.5028 - val_acc: 0.7835\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.78350, saving model to model_rnn.hdf5\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 287s 36ms/step - loss: 0.4174 - acc: 0.8199 - val_loss: 0.4513 - val_acc: 0.7880\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.78350 to 0.78800, saving model to model_rnn.hdf5\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 284s 35ms/step - loss: 0.3692 - acc: 0.8447 - val_loss: 0.4682 - val_acc: 0.7955\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.78800 to 0.79550, saving model to model_rnn.hdf5\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 284s 35ms/step - loss: 0.3242 - acc: 0.8681 - val_loss: 0.5396 - val_acc: 0.7875\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.79550\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 290s 36ms/step - loss: 0.2696 - acc: 0.8985 - val_loss: 0.5580 - val_acc: 0.7870\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.79550\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 289s 36ms/step - loss: 0.2199 - acc: 0.9209 - val_loss: 0.6588 - val_acc: 0.7770\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.79550\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 286s 36ms/step - loss: 0.1640 - acc: 0.9442 - val_loss: 0.8044 - val_acc: 0.7830\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.79550\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 286s 36ms/step - loss: 0.1214 - acc: 0.9610 - val_loss: 1.0227 - val_acc: 0.7725\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.79550\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 285s 36ms/step - loss: 0.0846 - acc: 0.9747 - val_loss: 1.1583 - val_acc: 0.7640\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.79550\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 286s 36ms/step - loss: 0.0636 - acc: 0.9835 - val_loss: 1.4662 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.79550\n"
     ]
    }
   ],
   "source": [
    "cp = ModelCheckpoint('model_rnn.hdf5',\n",
    "                     monitor='val_acc',\n",
    "                     verbose=1\n",
    "                     ,save_best_only=True)\n",
    "history = model.fit(x_train, y_train,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    epochs=10, batch_size=2,\n",
    "                    callbacks=[cp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a reccurrent neural network and GloVe word embeddings, the accuracy of predicting the sentiment for tweets increased to 79.6%. Although this cant be compared apples to apples because a different train/test split was employed for the recurrent neural net as was for the MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "Taking the same word embeddings and train/test split, we will train a convolutional neural network to create a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified convolutional neural network\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        (None, 117)               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 117, 100)          1962100   \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 116, 128)          25728     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 58, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 54, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 6, 128)            82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 2,168,694\n",
      "Trainable params: 2,168,694\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_cov1= Conv1D(128, 2, activation='relu')(embedded_sequences)\n",
    "l_pool1 = MaxPooling1D(2)(l_cov1)\n",
    "l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(5)(l_cov2)\n",
    "l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\n",
    "l_pool3 = MaxPooling1D(5)(l_cov3)  # global max pooling\n",
    "l_flat = Flatten()(l_pool3)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "preds = Dense(2, activation='softmax')(l_dense)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"Simplified convolutional neural network\")\n",
    "model.summary()\n",
    "cp=ModelCheckpoint('model_cnn.hdf5',monitor='val_acc',verbose=1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.7014 - acc: 0.5721 - val_loss: 0.8546 - val_acc: 0.5900\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.59000, saving model to model_cnn.hdf5\n",
      "Epoch 2/5\n",
      "8000/8000 [==============================] - 78s 10ms/step - loss: 0.6929 - acc: 0.5791 - val_loss: 0.6727 - val_acc: 0.6020\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.59000 to 0.60200, saving model to model_cnn.hdf5\n",
      "Epoch 3/5\n",
      "8000/8000 [==============================] - 78s 10ms/step - loss: 0.6820 - acc: 0.5939 - val_loss: 0.7396 - val_acc: 0.5995\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.60200\n",
      "Epoch 4/5\n",
      "8000/8000 [==============================] - 77s 10ms/step - loss: 0.6904 - acc: 0.6077 - val_loss: 0.7094 - val_acc: 0.5960\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.60200\n",
      "Epoch 5/5\n",
      "8000/8000 [==============================] - 77s 10ms/step - loss: 0.6997 - acc: 0.6127 - val_loss: 0.7690 - val_acc: 0.5890\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.60200\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train, y_train, \n",
    "                  validation_data=(x_val, y_val),\n",
    "                  epochs=5, batch_size=2,callbacks=[cp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Network did not improve the accuracy; mostly likely because I don't know how to use one and toggle with the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
