{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at Amazon's robots.txt file (or Twitter's, or Facebook's), you may be surprised to see them prohibit or severely restrict scraping.  Aren't there a lot of projects online using Twitter data?  And how dare they keep all that delicious, delicious information to themselves?  But before you start setting `'ROBOTSTXT_OBEY' = False`, read on!\n",
    "\n",
    "Most of The Big Websites (Google, Facebook, Twitter, etc) have APIs that allow you to access their information programmatically without using webpages.  This is good for both you and the website.  With an API, you can ask the server to send you only the specific information you want, without having to retrieve, filter out, and discard the CSS, HTML, PHP, and other code from the website.  This minimizes demand on the server and speeds up your task.  \n",
    "\n",
    "APIs typically include their own throttling to keep you from overloading the server, usually done by limiting the number of server requests per hour to a certain number.  \n",
    "\n",
    "To access an API, you will usually need an API key or token that uniquely identifies you.  This lets the company or service providing the API keep an eye on your usage and track what you are doing.  Different API keys can also be associated with different levels of authorization and access, so they work as a data security measure.  Keys or tokens may also be set to expire after a certain amount of time or number of uses.\n",
    "\n",
    "## Anatomy of an API\n",
    "\n",
    "*Access*- You request a key.  Your program provides the key with each API call, and it determines what your program can do in the API.  \n",
    "*Requests*- Your program requests the data you want with a call to the API.  The request will be made up of a method (type of query, using language defined by the API) and parameters (refine the query).  \n",
    "*Response*- The data returned by the API, usually in a common format such as JSON that your program can parse.  \n",
    "\n",
    "The specific syntax for each of these elements, and the format of the response, will vary from API to API.  In addition, APIs vary widely in their level of documentation and ease of use.  Before diving too deeply into an API-scraping project, do some judicious googling and if you see a lot of posts [like this one](https://mollyrocket.com/casey/stream_0029.html) consider going elsewhere.  Not all websites put their APIs front-and-center (did you know there are APIs for [NASA](https://api.nasa.gov/), [Marvel Comics](http://developer.marvel.com/), and [Star Wars](https://swapi.co/)?) so google will be your friend there as well.\n",
    "\n",
    "## Basics of API Queries: Wikipedia's API\n",
    "\n",
    "The process of using an API sounds a lot like scraping (make request, get response), but with an occasional added authorization layer.  Scrapy can handle authorization, so we can use it to access APIs too.\n",
    "\n",
    "That said, the first API we'll pull from is [Wikipedia's](https://www.mediawiki.org/wiki/API:Main_page), which doesn't require an authorization key.  Aside from needing to master the API's language, you'll find that using scrapy with an API is very similar to using scrapy on a website.\n",
    "\n",
    "We want to know what other entries on Wikipedia link to the [Monty Python](https://en.wikipedia.org/wiki/Monty_Python) page.  To do this, we can build a query using the [Wikipedia API Sandbox](https://en.wikipedia.org/wiki/Special:ApiSandbox).  Someone who is comfortable with the MediaWiki API syntax wouldn't need to use the sandbox, but for beginners it is very handy.  Note that API queries are nothing like SQL queries in syntax, despite their shared name.\n",
    "\n",
    "The query we will use looks like this:\n",
    "`https://en.wikipedia.org/w/api.php?action=query&format=xml&prop=linkshere&titles=Monty_Python&lhprop=title%7Credirect`\n",
    "\n",
    "Let's break that down into it's components:\n",
    "\n",
    "* `w/api.php`\n",
    "    * Tells the server that we are using the API to pull info, rather than scraping the raw pages.  \n",
    "    \n",
    "* `action=query`   \n",
    "    * We want information from the API (as opposed to changing information in the API)  \n",
    "    \n",
    "* `format=xml`  \n",
    "    * Format the return in xml- then we will parse it with xpath  \n",
    "    \n",
    "* `prop=linkshere`  \n",
    "    * We are interested in which pages link to our target page \n",
    "    \n",
    "* `titles=Monty_Python`  \n",
    "    * The target page is the Monty Python page.  Note that we used the exact name of the wikipedia page (Monty_Python).  \n",
    "    \n",
    "* `lhprop=title`  \n",
    "    * From those links, we want the title of each page  \n",
    "    \n",
    "* `redirect`  \n",
    "    * We also want to know if that link is a redirect  \n",
    "    \n",
    "\n",
    "The syntax of the MediaWiki API is based on php, thus the inclusion of `?` and `&` in the query.\n",
    "\n",
    "For most of the query elements, we could have passed multiple arguments.  For example, we could request the URL as well as the title of the linking pages, or asked for all the pages that link to Monty_Python and to Monty_Python's_Flying_Circus.  \n",
    "\n",
    "A query like this highlights why APIs are so handy.  Without an API, to find out the name of every page on Wikipedia that links to the Monty Python page we would have to scrape every single one of the 5,000,000+ articles in the English-language Wikipedia.  \n",
    "\n",
    "If you haven't done so already, click on the query link above and see what it returns.\n",
    "\n",
    "\n",
    "\n",
    "## Why use Scrapy for API calls\n",
    "\n",
    "For some API calls, scrapy would be overkill.  If you know that your query can be answered in one response, then you don't need scrapy- you can use the `requests` library to make your API call and a library like `lxml` to parse the return.\n",
    "\n",
    "The Wikipedia API, however, will only return ten items at a time in response to a query.  This sort of limitation is common to APIs to avoid overwhelming the server.  We can use scrapy to iterate over query results the same way that we iterated over the pages of the EverydaySexism website. \n",
    "\n",
    "Let's see the Wikipedia API and scrapy in action:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 links extracted!\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class WikiSpider(scrapy.Spider):\n",
    "    name = \"WS\"\n",
    "    \n",
    "    # Here is where we insert our API call.\n",
    "    start_urls = [\n",
    "        'https://en.wikipedia.org/w/api.php?action=query&format=xml&prop=linkshere&titles=Monty_Python&lhprop=title%7Credirect'\n",
    "        ]\n",
    "\n",
    "    # Identifying the information we want from the query response and extracting it using xpath.\n",
    "    def parse(self, response):\n",
    "        for item in response.xpath('//lh'):\n",
    "            # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "            # Other codes indicate links from 'Talk' pages, etc.  Since we are only interested in entries, we filter:\n",
    "            if item.xpath('@ns').extract_first() == '0':\n",
    "                yield {\n",
    "                    'title': item.xpath('@title').extract_first() \n",
    "                    }\n",
    "        # Getting the information needed to continue to the next ten entries.\n",
    "        next_page = response.xpath('continue/@lhcontinue').extract_first()\n",
    "        \n",
    "        # Recursively calling the spider to process the next ten entries, if they exist.\n",
    "        if next_page is not None:\n",
    "            next_page = '{}&lhcontinue={}'.format(self.start_urls[0],next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "            \n",
    "    \n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'PythonLinks.json',\n",
    "    # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "    'ROBOTSTXT_OBEY': False,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'LOG_ENABLED': False,\n",
    "    # We use CLOSESPIDER_PAGECOUNT to limit our scraper to the first 100 links.    \n",
    "    'CLOSESPIDER_PAGECOUNT' : 10\n",
    "})\n",
    "                                         \n",
    "\n",
    "# Starting the crawler with our spider.\n",
    "process.crawl(WikiSpider)\n",
    "process.start()\n",
    "print('First 100 links extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92, 1)\n",
      "                        title\n",
      "87               Hans Moleman\n",
      "88              Ripping Yarns\n",
      "89  List of British comedians\n",
      "90         Wensleydale cheese\n",
      "91              Art Garfunkel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Checking whether we got data \n",
    "\n",
    "Monty=pd.read_json('https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/PythonLinks.json', orient='records')\n",
    "print(Monty.shape)\n",
    "print(Monty.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up\n",
    "\n",
    "Our API call was successful.  While we examined 100 links, we only saved 92 (the others weren't links from entry pages).  \n",
    "\n",
    "We've barely scraped (pun intended) the surface of what scrapy and APIs can do.  Scrapy has changed a lot in the years since its debut, so when googling make sure the answers you see are from 2015 at the latest-- otherwise you'll likely not be able to use the code.  \n",
    "\n",
    "Back to the issue of authorization keys- often the key is simply included in the query string as an additional arguments.  In other cases, if you need your scraper to be able to enter a key or login information into a form, scrapy [has you covered](http://stackoverflow.com/questions/30102199/form-authentication-login-a-site-using-scrapy).  \n",
    "\n",
    "There's a lot of fun to be had in scraping and APIs-- it's a way to feel like you're getting a lot of information with very little effort!  Beware, however.  You're not getting information at all.  Scraping gives you *data*, an undifferentiated mess of bytes with no compelling meaning on its own.  Think of that list of Wiki entries that link to Monty Python.  It's cool that we could get it, but what does it mean?  Your job as a data scientist is to convert *data* to *information*-- something people can use to make decisions or understand the world.  Modeling data to get information is hard but worthwhile work, and its those kinds of projects that will really build your portfolio as you go on the market.  \n",
    "\n",
    "That said, scraping up some original data can provide the *foundation* for an interesting and original final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Do a little scraping or API-calling of your own.  Pick a new website and see what you can get out of it.  Expect that you'll run into bugs and blind alleys, and rely on your mentor to help you get through.  \n",
    "\n",
    "Formally, your goal is to write a scraper that will:\n",
    "\n",
    "1) Return specific pieces of information (rather than just downloading a whole page)  \n",
    "2) Iterate over multiple pages/queries  \n",
    "3) Save the data to your computer  \n",
    "\n",
    "Once you have your data, compute some statistical summaries and/or visualizations that give you some new insights into your scraping topic of interest.  Write up a report from scraping code to summary and share it with your mentor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Importing in each cell because of the kernel restarts.\n",
    "\n",
    "#First Iteration: Scraping only the Titles\n",
    "\n",
    "import scrapy\n",
    "import re\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class BASpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"BAS\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://www.bonappetit.com/basically',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every <article> element on the page.\n",
    "        for article in response.xpath('//*[@id=\"react-app\"]/div/div[2]/div/div[@class=\"basically__home__card basically__home__card--full basically__home__card--standard\"]'):\n",
    "            \n",
    "            # Yield a dictionary with the values we want.\n",
    "            yield {\n",
    "                \n",
    "                'title': article.xpath('span/h2/a/span/text()').extract()\n",
    "                'link'\n",
    "\n",
    "                \n",
    "                #'name': article.xpath('header/h2/a/@title').extract_first(),\n",
    "                #'date': article.xpath('header/section/span[@class=\"entry-date\"]/text()').extract_first(),\n",
    "                #'text': article.xpath('section[@class=\"entry-content\"]/p/text()').extract(),\n",
    "                #'tags': article.xpath('*/span[@class=\"tag-links\"]/a/text()').extract()\n",
    "            }\n",
    "        # Get the URL of the previous page.\n",
    "#        next_page = response.xpath('//div[@class=\"nav-previous\"]/a/@href').extract_first()\n",
    "        \n",
    "        # There are a LOT of pages here.  For our example, we'll just scrape the first 9.\n",
    "        # This finds the page number. The next segment of code prevents us from going beyond page 9.\n",
    "#        pagenum = int(re.findall(r'\\d+',next_page)[0])\n",
    "        \n",
    "        # Recursively call the spider to run on the next page, if it exists.\n",
    "#        if next_page is not None and pagenum < 10:\n",
    "#            next_page = response.urljoin(next_page)\n",
    "            # Request the next page and recursively parse it the same way we did above\n",
    "#            yield scrapy.Request(next_page, callback=self.parse)\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "# The new settings have to do with scraping etiquette.          \n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'Basically.json',       # Name our storage file.\n",
    "    'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "    'ROBOTSTXT_OBEY': True,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(BASpider)\n",
    "process.start()\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "### Specific Article\n",
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "import re\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class BAArtSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"BAArt\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://www.bonappetit.com/story/what-is-a-waxy-potato',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every <article> element on the page.\n",
    "        article = response.xpath('//*[@id=\"main-content\"]/article')\n",
    "            \n",
    "        # Yield a dictionary with the values we want.\n",
    "        yield {\n",
    "\n",
    "            'title': article.xpath('div[1]/header/div/div[1]/h1/text()').extract(),\n",
    "            'subtitle': article.xpath('div[1]/header/div/div[2]/p/text()').extract(),\n",
    "            'contributor': article.xpath('div[1]/header/div/div[2]/div[1]/div/div/div/div/a/text()').extract(),\n",
    "            'date': article.xpath('div[1]/header/div/div[2]/div[1]/div/time/text()').extract(),\n",
    "            'text': \" \".join(article.xpath('div[2]/div/div[1]/div/div[1]//text()').extract())\n",
    "            \n",
    "            #'name': article.xpath('header/h2/a/@title').extract_first(),\n",
    "            #'date': article.xpath('header/section/span[@class=\"entry-date\"]/text()').extract_first(),\n",
    "            #'text': article.xpath('section[@class=\"entry-content\"]/p/text()').extract(),\n",
    "            #'tags': article.xpath('*/span[@class=\"tag-links\"]/a/text()').extract()\n",
    "        }\n",
    "        # Get the URL of the previous page.\n",
    "#        next_page = response.xpath('//div[@class=\"nav-previous\"]/a/@href').extract_first()\n",
    "        \n",
    "        # There are a LOT of pages here.  For our example, we'll just scrape the first 9.\n",
    "        # This finds the page number. The next segment of code prevents us from going beyond page 9.\n",
    "#        pagenum = int(re.findall(r'\\d+',next_page)[0])\n",
    "        \n",
    "        # Recursively call the spider to run on the next page, if it exists.\n",
    "#        if next_page is not None and pagenum < 10:\n",
    "#            next_page = response.urljoin(next_page)\n",
    "            # Request the next page and recursively parse it the same way we did above\n",
    "#            yield scrapy.Request(next_page, callback=self.parse)\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "# The new settings have to do with scraping etiquette.          \n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'BasicallyPotatoe.json',       # Name our storage file.\n",
    "    'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "    'ROBOTSTXT_OBEY': True,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(BAArtSpider)\n",
    "process.start()\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "### Specific Article\n",
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "import re\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class BasicallySpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"Basically\"\n",
    "\n",
    "    # URL(s) to start with.\n",
    "    start_urls = ['https://www.bonappetit.com/basically']\n",
    "    BASE_URL = 'https://www.bonappetit.com/'\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        links = response.xpath('//*[@id=\"react-app\"]/div/div[2]/div/div/span/h2/a/@href').extract()\n",
    "        for link in links:\n",
    "            absolute_url = self.BASE_URL + like\n",
    "            yield scrapy.Request(absolute_url, callback = self.parse_attr)\n",
    "        \n",
    "    def parse_attr(self, response):\n",
    "        # Iterate over every <article> element on the page.\n",
    "        article = response.xpath('//*[@id=\"main-content\"]/article')\n",
    "            \n",
    "        # Yield a dictionary with the values we want.\n",
    "        yield {\n",
    "\n",
    "            'title': article.xpath('div[1]/header/div/div[1]/h1/text()').extract(),\n",
    "            'subtitle': article.xpath('div[1]/header/div/div[2]/p/text()').extract(),\n",
    "            'contributor': article.xpath('div[1]/header/div/div[2]/div[1]/div/div/div/div/a/text()').extract(),\n",
    "            'date': article.xpath('div[1]/header/div/div[2]/div[1]/div/time/text()').extract(),\n",
    "            'text': \" \".join(article.xpath('div[2]/div/div[1]/div/div[1]//text()').extract())\n",
    "            \n",
    "             }\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "# The new settings have to do with scraping etiquette.          \n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',                  # Store data in JSON format.\n",
    "    'FEED_URI': 'BasicallyArticles.json',   # Name our storage file.\n",
    "    'LOG_ENABLED': False,                   # Turn off logging for now.\n",
    "    'ROBOTSTXT_OBEY': True,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(BasicallySpider)\n",
    "process.start()\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "### The Final Round! This one actually worked.\n",
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "import re\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class BasicItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    link = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    subtitle = scrapy.Field()\n",
    "    contributor = scrapy.Field()\n",
    "    date = scrapy.Field()\n",
    "    text = scrapy.Field()\n",
    "\n",
    "class BasicallySpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"Basically\"\n",
    "\n",
    "    # URL(s) to start with.\n",
    "    start_urls = ['https://www.bonappetit.com/basically']\n",
    "    BASE_URL = 'https://www.bonappetit.com/'\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        links = response.xpath('//*[@id=\"react-app\"]/div/div[2]/div/div/span/h2/a/@href').extract()\n",
    "        for link in links:\n",
    "            absolute_url = self.BASE_URL + link\n",
    "            yield scrapy.Request(absolute_url, callback = self.parse_attr)\n",
    "        \n",
    "    def parse_attr(self, response):\n",
    "        article = response.xpath('//*[@id=\"main-content\"]/article')\n",
    "        item = BasicItem()\n",
    "        item['link'] = response.url\n",
    "        item['title'] = article.xpath('div[1]/header/div/div[1]/h1/text()').extract(),\n",
    "        item['subtitle'] = article.xpath('div[1]/header/div/div[2]/p/text()').extract(),\n",
    "        item['contributor'] =  article.xpath('div[1]/header/div/div[2]/div[1]/div/div/div/div/a/text()').extract(),\n",
    "        item['date'] = article.xpath('div[1]/header/div/div[2]/div[1]/div/time/text()').extract(),\n",
    "        item['text'] = \" \".join(article.xpath('div[2]/div/div[1]/div/div[1]//text()').extract())\n",
    "        return item\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "# The new settings have to do with scraping etiquette.          \n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',                  # Store data in JSON format.\n",
    "    'FEED_URI': 'BasicallyArticles.json',   # Name our storage file.\n",
    "    'LOG_ENABLED': False,                   # Turn off logging for now.\n",
    "    'ROBOTSTXT_OBEY': True,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(BasicallySpider)\n",
    "process.start()\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contributor</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[Christina Chae]]</td>\n",
       "      <td>[[September 13, 2019]]</td>\n",
       "      <td>https://www.bonappetit.com/story/should-you-bu...</td>\n",
       "      <td>[[What’s it good for? How do you use it? And m...</td>\n",
       "      <td>I didn’t think of myself as an “appliance junk...</td>\n",
       "      <td>[[Real Talk: Should You Buy a Food Processor?]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[Al Cullito]]</td>\n",
       "      <td>[[June 6, 2019]]</td>\n",
       "      <td>https://www.bonappetit.com/story/happy-hour-wi...</td>\n",
       "      <td>[[This refreshing, low-ABV cocktail is what Ju...</td>\n",
       "      <td>Tired of cocktail recipes that call for expens...</td>\n",
       "      <td>[[A Pimm's Cup Recipe to Start Summer Off Right]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[Elyse Inamin]]</td>\n",
       "      <td>[[June 7, 2019]]</td>\n",
       "      <td>https://www.bonappetit.com/story/never-fail-ki...</td>\n",
       "      <td>[[You can't go wrong with spaghetti tossed wit...</td>\n",
       "      <td>Welcome to  Never Fail,  a weekly column where...</td>\n",
       "      <td>[[The Kinda-Healthy Kale Pasta I Can Always Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[Molly Ba]]</td>\n",
       "      <td>[[June 11, 2019]]</td>\n",
       "      <td>https://www.bonappetit.com/story/mediocre-berr...</td>\n",
       "      <td>[[Not all berries are created equal, but these...</td>\n",
       "      <td>If you follow any  Bon Appétit  staffer on Ins...</td>\n",
       "      <td>[[These 2 Ingredients Make Even Mediocre Berri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[Alex Delan]]</td>\n",
       "      <td>[[June 14, 2019]]</td>\n",
       "      <td>https://www.bonappetit.com/story/better-whippe...</td>\n",
       "      <td>[[Your whipped cream could be even better. (Ye...</td>\n",
       "      <td>Some would call us big whipped cream people ov...</td>\n",
       "      <td>[[This One Ingredient Takes Whipped Cream From...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          contributor                    date  \\\n",
       "0  [[Christina Chae]]  [[September 13, 2019]]   \n",
       "1      [[Al Cullito]]        [[June 6, 2019]]   \n",
       "2    [[Elyse Inamin]]        [[June 7, 2019]]   \n",
       "3        [[Molly Ba]]       [[June 11, 2019]]   \n",
       "4      [[Alex Delan]]       [[June 14, 2019]]   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.bonappetit.com/story/should-you-bu...   \n",
       "1  https://www.bonappetit.com/story/happy-hour-wi...   \n",
       "2  https://www.bonappetit.com/story/never-fail-ki...   \n",
       "3  https://www.bonappetit.com/story/mediocre-berr...   \n",
       "4  https://www.bonappetit.com/story/better-whippe...   \n",
       "\n",
       "                                            subtitle  \\\n",
       "0  [[What’s it good for? How do you use it? And m...   \n",
       "1  [[This refreshing, low-ABV cocktail is what Ju...   \n",
       "2  [[You can't go wrong with spaghetti tossed wit...   \n",
       "3  [[Not all berries are created equal, but these...   \n",
       "4  [[Your whipped cream could be even better. (Ye...   \n",
       "\n",
       "                                                text  \\\n",
       "0  I didn’t think of myself as an “appliance junk...   \n",
       "1  Tired of cocktail recipes that call for expens...   \n",
       "2  Welcome to  Never Fail,  a weekly column where...   \n",
       "3  If you follow any  Bon Appétit  staffer on Ins...   \n",
       "4  Some would call us big whipped cream people ov...   \n",
       "\n",
       "                                               title  \n",
       "0    [[Real Talk: Should You Buy a Food Processor?]]  \n",
       "1  [[A Pimm's Cup Recipe to Start Summer Off Right]]  \n",
       "2  [[The Kinda-Healthy Kale Pasta I Can Always Co...  \n",
       "3  [[These 2 Ingredients Make Even Mediocre Berri...  \n",
       "4  [[This One Ingredient Takes Whipped Cream From...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "basic_df = pd.read_json('BasicallyArticles.json')\n",
    "basic_df[basic_df['contributor'] == '[[Sarah Jampel]]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I didn’t think of myself as an “appliance junkie” before I moved into my current apartment. By Brooklyn standards, it feels luxuriously large, like I could run a restaurant supply store out of the kitchen. As it turns out, all I really needed to change my tune was simply more square-footage: I’m now the co-owner of, among other things, a food processor that I’m surprised to find myself reaching for all the time. After a year of chopping, slicing, shredding, and puréeing, I have some thoughts on whether you, too, should own a food processor. First, let’s talk about what a food processor even  is . At minimum, a basic food processor comes with a bowl, a removable lid, a base (which contains the motor), and a very sharp blade. It’s more or less an extremely powerful knife that excels at quickly chopping and grinding tons of different ingredients, from onions to to nuts to hard cheeses, in a matter of seconds. The set of attachments will shred and slice carrots ( carrot cake !), cabbage ( slaw !), or potatoes ( hash browns !). There are a whole host of fancy-pants attachments that can knead bread dough and juice vegetables and fold your laundry and pick up your children from school, but for the purposes of this article, I’ll be talking about the no-frills basic kind. Homemade granola bars , thanks to your food processor. \\n Photo by Emma Fishman, Food Styling by Sue Li I’m going to be upfront here: Unless you’re the kind of cook who regularly makes your own homemade nut butters and  energy balls , you don’t absolutely  need  a food processor (I point out these two foods specifically because I’m convinced you can’t make them at home without one). However, if you do a lot of meal prep or you’re chopping-averse, deathly afraid of sharp tools like mandolines and box graters, or uncertain about your knife skills, a food processor might be a worthwhile purchase to make cooking faster and generally more enjoyable. I find myself pulling out my food processor at least once a week to make my own  hummus  or  bean dips , which I do frequently, as well as to whip together sauces and dips that are meant to have a bit of texture. Because most food processors come with a “pulse” feature that allows you to control how finely chopped up your ingredients get, they’re excellent for making not-quite-smooth-in-a-good-way sauces like  pesto  (BA’s Best) and other all-purpose  green sauces ,  romesco ,  salsa , or  nam prik , the chunky Thai chile dip. (Conversely, food processors are  not  great at achieving a uniformly smooth texture, so if you're primarily interested in making silky soups or smoothies and only have room for one appliance, a high-powered blender is the better bet.) Advertisement You're going to need a food processor for  cauliflower rice . \\n Photo by Stephen Kent Johnson, food styling by Rebecca Jurkevich, prop styling by Kalen Kaminski There are a host of other foods I’ll make less occasionally, but when I do, I’m extremely thankful to have a food processor on hand to get the job done, as it does everything so much quicker than I could do by hand. Take  Lauren Schaefer ’s  Cauliflower Rice Pilaf , for example. Owning a food processor means I don’t have to finely chop an entire head of cauliflower with my chef knife. And I can make  falafel  at home: First, I chop up the herbs, then I wipe out the bowl and add the soaked chickpeas. There’s no need to dirty a cutting board or a knife. When I’m in the mood to bake, my food processor effortlessly cuts cold butter into flour for flaky  pie dough  and tall, buttery  biscuits . It finely crushes graham crackers for a press-in pie crust (hellooo,  frozen margarita pie ) and makes breadcrumbs—coarse  or  fine, you choose—for topping  pastas  or a bowl of  brothy beans . And on the days when I make a couple of gallons of homemade kimchi, it’s a godsend for finely chopping large quantities of garlic and ginger, which would take an eternity to mince by hand. Could I do all of these tasks without a food processor? Technically, yes—but I’d be a much slower (and grumpier) cook for it. A food processor will get you perfectly bitsy breadcrumbs, ideal for  pasta . \\n Michael Graydon & Nikole Herriott Advertisement Ultimately the most important question that should drive your decision to buy a food processor is: Will you use it? If you can honestly answer “yes” to that question, all that’s left to consider is whether it’s worth the valuable storage or counter space it will take up in your kitchen—and, yeah, the money. A good food processor will last you a long time, but it’s not cheap. (The Test Kitchen’s preferred model, the  Cuisinart 14-cup , retails for about $160.) If that’s more than you’re willing to spend (or if you’re just looking to avoid hand-chopping the occasional onion or chile), a less-expensive mini food processor (like our  favorite model ) might be more up your alley, especially if you have a smaller kitchen. I know the time may come when I have to leave my palatial kitchen behind for a standard closet-sized one. But I’m taking my food processor with me—even if that means I have to store it in my bedroom. Buy it: Cuisinart 14-Cup, $156 on amazon.com All products featured on Bonappetit.com are independently selected by our editors. However, when you buy something through our retail links, we may earn an affiliate commission.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "contributor_list = [contributor[0][0] for contributor in basic_df['contributor']]\n",
    "#pd.Series(contributor_list).unique()\n",
    "basic_df['contributor'] = contributor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contributor</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Christina Chae</td>\n",
       "      <td>[[September 13, 2019]]</td>\n",
       "      <td>https://www.bonappetit.com/story/should-you-bu...</td>\n",
       "      <td>[[What’s it good for? How do you use it? And m...</td>\n",
       "      <td>I didn’t think of myself as an “appliance junk...</td>\n",
       "      <td>[[Real Talk: Should You Buy a Food Processor?]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Al Cullito</td>\n",
       "      <td>[[June 6, 2019]]</td>\n",
       "      <td>https://www.bonappetit.com/story/happy-hour-wi...</td>\n",
       "      <td>[[This refreshing, low-ABV cocktail is what Ju...</td>\n",
       "      <td>Tired of cocktail recipes that call for expens...</td>\n",
       "      <td>[[A Pimm's Cup Recipe to Start Summer Off Right]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Elyse Inamin</td>\n",
       "      <td>[[June 7, 2019]]</td>\n",
       "      <td>https://www.bonappetit.com/story/never-fail-ki...</td>\n",
       "      <td>[[You can't go wrong with spaghetti tossed wit...</td>\n",
       "      <td>Welcome to  Never Fail,  a weekly column where...</td>\n",
       "      <td>[[The Kinda-Healthy Kale Pasta I Can Always Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Molly Ba</td>\n",
       "      <td>[[June 11, 2019]]</td>\n",
       "      <td>https://www.bonappetit.com/story/mediocre-berr...</td>\n",
       "      <td>[[Not all berries are created equal, but these...</td>\n",
       "      <td>If you follow any  Bon Appétit  staffer on Ins...</td>\n",
       "      <td>[[These 2 Ingredients Make Even Mediocre Berri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alex Delan</td>\n",
       "      <td>[[June 14, 2019]]</td>\n",
       "      <td>https://www.bonappetit.com/story/better-whippe...</td>\n",
       "      <td>[[Your whipped cream could be even better. (Ye...</td>\n",
       "      <td>Some would call us big whipped cream people ov...</td>\n",
       "      <td>[[This One Ingredient Takes Whipped Cream From...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      contributor                    date  \\\n",
       "0  Christina Chae  [[September 13, 2019]]   \n",
       "1      Al Cullito        [[June 6, 2019]]   \n",
       "2    Elyse Inamin        [[June 7, 2019]]   \n",
       "3        Molly Ba       [[June 11, 2019]]   \n",
       "4      Alex Delan       [[June 14, 2019]]   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.bonappetit.com/story/should-you-bu...   \n",
       "1  https://www.bonappetit.com/story/happy-hour-wi...   \n",
       "2  https://www.bonappetit.com/story/never-fail-ki...   \n",
       "3  https://www.bonappetit.com/story/mediocre-berr...   \n",
       "4  https://www.bonappetit.com/story/better-whippe...   \n",
       "\n",
       "                                            subtitle  \\\n",
       "0  [[What’s it good for? How do you use it? And m...   \n",
       "1  [[This refreshing, low-ABV cocktail is what Ju...   \n",
       "2  [[You can't go wrong with spaghetti tossed wit...   \n",
       "3  [[Not all berries are created equal, but these...   \n",
       "4  [[Your whipped cream could be even better. (Ye...   \n",
       "\n",
       "                                                text  \\\n",
       "0  I didn’t think of myself as an “appliance junk...   \n",
       "1  Tired of cocktail recipes that call for expens...   \n",
       "2  Welcome to  Never Fail,  a weekly column where...   \n",
       "3  If you follow any  Bon Appétit  staffer on Ins...   \n",
       "4  Some would call us big whipped cream people ov...   \n",
       "\n",
       "                                               title  \n",
       "0    [[Real Talk: Should You Buy a Food Processor?]]  \n",
       "1  [[A Pimm's Cup Recipe to Start Summer Off Right]]  \n",
       "2  [[The Kinda-Healthy Kale Pasta I Can Always Co...  \n",
       "3  [[These 2 Ingredients Make Even Mediocre Berri...  \n",
       "4  [[This One Ingredient Takes Whipped Cream From...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here’s a thought: Would life would be (just a tad bit) easier if, at the grocery store, potatoes were divided up into the “waxy,” “floury,” and “in-between” categories? If you’ve ever bought a couple of russets thinking you were going to make potato salad, or a pound of fingerlings thinking they’d make a really unique mashed potato situation… then you feel my pain. Waxy and floury potatoes are not interchangeable—your gloppy potato salad or gluey mash will have alerted you to that truth. But what do these categories even mean, which type is best for what use, and what common potatoes fall in each group?! And is there a kind of potato that will almost surely work in any situation? You have questions, and we have answers: Waxy New potatoes , Red Bliss, pee wees, fingerlings! These potatoes, which are often small (and apparently, fun-named?), have thin, smooth skin and creamy, almost shiny flesh. They’re also known for their particularly potato-forward flavor. Because waxy potatoes are relatively low in starch and high in moisture, their cells stay intact when they’re cooked, meaning that slices or cubes hold up when boiled or baked. Waxy potatoes will hold their shape when boiled—and dressed with a ton of  cheese and pepper . \\n Eva Kolenko This makes waxy varieties the best choice for dishes where you want the potatoes to maintain their shape: Think  potato salad ,  gratin ,  smashed crispy potatoes ,  cacio e pepe potatoes , or a tray of  salt-roasted spuds . But beware: They won’t break down into a creamy spoonable mash, and they’re not great for that  classic baked  and butter-topped beauty.: The interior ofr a waxy potato is firm (even a bit squeaky) rather than fluffy or yielding. Floury Higher in starch and lower in moisture than waxy potatoes, the floury varieties, like matte-skinned russets and Idahos, fall apart when boiled (the starches harden and expand, causing the skin to split and the interior to crumble into meal). Floury potatoes are starchy, which means they crisp up nicely in  rosti  and latkes. \\n © 2015 Christopher Testani Advertisement This fragility makes floury potatoes ideal for adding dairy-free creaminess to  puréed soups  or providing the elusive airy, whipped texture to a  mash  (though not alone: we favor  mashing them with a waxier variety  for optimal flavor). And since starchiness often translates into crispiness, floury potatoes are good candidates for  roasting  and frying: Use them in  French fries ,  latkes ,  hash browns , or  rosti  for a delightfully crispy shell and creamy insides. In-betweenies And finally, there are the “all-purpose” potatoes, which fall somewhere between the waxy and floury poles. If, by the number of times you’ve seen  Yukon Golds  in a  BA  recipe, you’ve guessed that these are our AP potatoes of choice, you are correct. Yukon Golds are our choice for  Perfectly Roasted Potatoes . \\n Ben Dewey Semi-starchy and semi-waxy, they’re simultaneously more plush than waxy potatoes and less crumbly than floury ones (which makes sense, since YGs are a cross between a starchy North American white potato and a waxier South American yellow one). Because they hold their shape when boiled, grated, or fried  and  yield to mashing, they’re incredibly versatile:  Roast them , smash them,  boil them , shred them into  latkes , or  break them down into creamy oblivion . What  can’t  this potato do?! In  Molly Baz ’s  Slow-Roast Gochujang Chicken , baby Yukon Golds sizzle in the chicken fat that drips off the bird as it cooks: By the time the meat is fall-apart tender, the potatoes are buttery-soft. You can smash into the schmaltz without sacrificing their cute globular shape. Now that you can tell a waxy potato apart from a floury one, you’re destined for never-starchy potato salads and deep, dark hashbrowns. But when in doubt, Yukon Gold. Chicken and potatoes, a match made in heaven: Slow-Roast Gochujang Chicken This isn’t the crisp-skinned, high-heat roast chicken you’re probably familiar with. Instead, it’s a melt-in-your-mouth tender, schmaltzy, slow-roast version that’s more similar to rotisserie chicken—except (bonus!) it gets slathered in the funky-spicy-sweet gochujang. And while the meat might be the star of the show, don't discount those buttery-soft, nearly-confited potatoes, which cook gently in the chicken fat. Makes you wonder why you haven't  always  been roasting long-cooking vegetables—carrots, cauliflower, turnips (?!), winter squash, fennel—under the bird for a built-in side, huh? View Recipe\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_df[basic_df['contributor'] == 'Sarah Jampe']['text'][148]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Christina Chae', 'Al Cullito', 'Elyse Inamin', 'Molly Ba',\n",
       "       'Alex Delan', 'Sarah Jampe', 'Carla Lalli Musi', 'Alison Roma',\n",
       "       'Amiel Stane', 'Aliza Abarbane', 'Emma Wartzma', 'Claire Saffit',\n",
       "       'Alyse Whitne', 'Julia Krame', 'Jesse Spark', 'Rochelle Bilo',\n",
       "       'Alex Begg', 'Emily Schult', 'Elaheh Nozar', 'Hilary Cadiga',\n",
       "       'Carey Poli', 'Amanda Shapir', 'Rachel Karte', 'Adam Rapopor',\n",
       "       'Meryl Rothstei', 'Alex Pastro'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_df['contributor'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149, 6)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successfully scraped the contents by category of 149 articles on the bon appetit 'basically' blog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
