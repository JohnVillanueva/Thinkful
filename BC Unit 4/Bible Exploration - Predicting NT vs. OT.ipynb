{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "#from collections import Counter\n",
    "#import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "bibleraw = pd.read_json('https://raw.githubusercontent.com/bibleapi/bibleapi-bibles-json/master/asv.json', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>book_name</th>\n",
       "      <th>chapter</th>\n",
       "      <th>text</th>\n",
       "      <th>translation_id</th>\n",
       "      <th>verse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gen</td>\n",
       "      <td>Genesis</td>\n",
       "      <td>1</td>\n",
       "      <td>In the beginning God created the heavens and t...</td>\n",
       "      <td>ASV</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gen</td>\n",
       "      <td>Genesis</td>\n",
       "      <td>1</td>\n",
       "      <td>And the earth was waste and void; and darkness...</td>\n",
       "      <td>ASV</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gen</td>\n",
       "      <td>Genesis</td>\n",
       "      <td>1</td>\n",
       "      <td>And God said, Let there be light: and there wa...</td>\n",
       "      <td>ASV</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gen</td>\n",
       "      <td>Genesis</td>\n",
       "      <td>1</td>\n",
       "      <td>And God saw the light, that it was good: and G...</td>\n",
       "      <td>ASV</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gen</td>\n",
       "      <td>Genesis</td>\n",
       "      <td>1</td>\n",
       "      <td>And God called the light Day, and the darkness...</td>\n",
       "      <td>ASV</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  book_id book_name  chapter  \\\n",
       "0     Gen   Genesis        1   \n",
       "1     Gen   Genesis        1   \n",
       "2     Gen   Genesis        1   \n",
       "3     Gen   Genesis        1   \n",
       "4     Gen   Genesis        1   \n",
       "\n",
       "                                                text translation_id  verse  \n",
       "0  In the beginning God created the heavens and t...            ASV      1  \n",
       "1  And the earth was waste and void; and darkness...            ASV      2  \n",
       "2  And God said, Let there be light: and there wa...            ASV      3  \n",
       "3  And God saw the light, that it was good: and G...            ASV      4  \n",
       "4  And God called the light Day, and the darkness...            ASV      5  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bibleraw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'بِسْمِ اللَّهِ الرَّحْمَنِ الرَّحِيمِ\\nالْحَمْدُ لِلَّهِ رَبِّ الْعَالَمِينَ\\nالرَّحْمَنِ الرَّحِيمِ\\nمَالِكِ يَوْمِ الدِّينِ\\nإِيَّاكَ نَعْبُدُ وَإِيَّاكَ نَسْتَعِينُ\\nاهْدِنَا الصِّرَاطَ الْمُسْتَقِيمَ\\nصِرَاطَ الَّذِينَ أَنْعَمْتَ عَلَيْهِمْ غَيْرِ الْمَغْضُوبِ عَلَيْهِمْ وَلَا الضَّالِّينَ\\nبِسْمِ اللَّهِ الرَّحْمَنِ الرَّحِيمِ الم\\nذَلِكَ الْكِتَابُ لَا رَيْبَ فِيهِ هُدًى لِلْمُتَّقِينَ\\nالَّذِينَ يُؤْمِنُونَ بِالْغَيْبِ وَيُقِيمُونَ الصَّلَاةَ وَمِمَّا رَزَقْنَاهُمْ يُنْفِقُونَ\\nوَالَّذِينَ يُؤْمِنُونَ بِمَا أُنْزِلَ إِلَيْكَ وَمَا أُنْزِلَ مِنْ قَبْلِكَ وَبِالْآخِرَةِ هُمْ يُوقِنُونَ\\nأُولَئِكَ عَلَى هُدًى مِنْ رَبِّهِمْ وَأُولَئِكَ هُمُ الْمُفْلِحُونَ\\nإِنَّ الَّذِينَ كَفَرُوا سَوَاءٌ عَلَيْهِمْ أَأَنْذَرْتَهُمْ أَمْ لَمْ تُنْذِرْهُمْ لَا يُؤْمِنُونَ\\nخَتَمَ اللَّهُ عَلَى قُلُوبِهِمْ وَعَلَى سَمْعِهِمْ وَعَلَى أَبْصَارِهِمْ غِشَاوَةٌ وَلَهُمْ عَذَابٌ عَظِيمٌ\\nوَمِنَ النَّاسِ مَنْ يَقُولُ آمَنَّا بِاللَّهِ وَبِالْيَوْمِ الْآخِرِ وَمَا هُمْ بِمُؤْمِنِينَ\\nيُخَادِعُونَ اللَّهَ وَالَّذِينَ آمَنُو'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accidentally download the quran in original arabic, unfortunately it can't be useful.\n",
    "quranraw = open('../__DATA__/Holy Texts/quran-simple.txt','r')\n",
    "quranraw = quranraw.read()\n",
    "quranraw[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "file_list = glob.glob(os.path.join(os.getcwd(), \"../__DATA__/Holy Texts/quran-verse-by-verse-text\", \"*.txt\"))\n",
    "file_list.sort()\n",
    "\n",
    "quran_verses = []\n",
    "\n",
    "for file_path in file_list:\n",
    "    with open(file_path) as f_input:\n",
    "        quran_verses.append(f_input.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ways of organizing the analysis:\n",
    "- We will be performing a model that classifies OT vs. NT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6349"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quran_verses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Quran\\nModern English Translation\\nVerse by Verse\\n\\nTranslated by Talal Itani\\nwww.ClearQuran.com\\n\\nAvailable in two editions. This edition (A) uses the word 'Allah' to refer to the Creator. Edition (B) uses the word 'God'.\\n\\nThese files can be shared and distributed\\nProvided under the Creative Commons License\\nAttribution-NonCommercial-NoDerivs\\n\\n\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quran_verses[6348]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#but it doesn't matter anyways because I just decided that I don't to use the Quran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Genesis', 'Exodus', 'Leviticus', 'Numbers', 'Deuteronomy',\n",
       "       'Joshua', 'Judges', 'Ruth', '1 Samuel', '2 Samuel', '1 Kings',\n",
       "       '2 Kings', '1 Chronicles', '2 Chronicles', 'Ezra', 'Nehemiah',\n",
       "       'Esther', 'Job', 'Psalms', 'Proverbs', 'Ecclesiastes',\n",
       "       'Song of Solomon', 'Isaiah', 'Jeremiah', 'Lamentations', 'Ezekiel',\n",
       "       'Daniel', 'Hosea', 'Joel', 'Amos', 'Obadiah', 'Jonah', 'Micah',\n",
       "       'Nahum', 'Habakkuk', 'Zephaniah', 'Haggai', 'Zechariah', 'Malachi',\n",
       "       'Matthew', 'Mark', 'Luke', 'John', 'Acts of the Apostles',\n",
       "       'Romans', '1 Corinthians', '2 Corinthians', 'Galatians',\n",
       "       'Ephesians', 'Philippians', 'Colossians', '1 Thessalonians',\n",
       "       '2 Thessalonians', '1 Timothy', '2 Timothy', 'Titus', 'Philemon',\n",
       "       'Hebrews', 'James', '1 Peter', '2 Peter', '1 John', '2 John',\n",
       "       '3 John', 'Jude', 'Revelation'], dtype=object)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bibleraw['book_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating list of Old Testament and New Testament Chapters\n",
    "OT = ['Genesis', 'Exodus', 'Leviticus', 'Numbers', 'Deuteronomy',\n",
    "       'Joshua', 'Judges', 'Ruth', '1 Samuel', '2 Samuel', '1 Kings',\n",
    "       '2 Kings', '1 Chronicles', '2 Chronicles', 'Ezra', 'Nehemiah',\n",
    "       'Esther', 'Job', 'Psalms', 'Proverbs', 'Ecclesiastes',\n",
    "       'Song of Solomon', 'Isaiah', 'Jeremiah', 'Lamentations', 'Ezekiel',\n",
    "       'Daniel', 'Hosea', 'Joel', 'Amos', 'Obadiah', 'Jonah', 'Micah',\n",
    "       'Nahum', 'Habakkuk', 'Zephaniah', 'Haggai', 'Zechariah', 'Malachi']\n",
    "NT = ['Matthew', 'Mark', 'Luke', 'John', 'Acts of the Apostles',\n",
    "       'Romans', '1 Corinthians', '2 Corinthians', 'Galatians',\n",
    "       'Ephesians', 'Philippians', 'Colossians', '1 Thessalonians',\n",
    "       '2 Thessalonians', '1 Timothy', '2 Timothy', 'Titus', 'Philemon',\n",
    "       'Hebrews', 'James', '1 Peter', '2 Peter', '1 John', '2 John',\n",
    "       '3 John', 'Jude', 'Revelation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible = bibleraw.copy()\n",
    "bible['testament'] = ''\n",
    "bible['book_chapter'] = ''\n",
    "bible.loc[bible['book_name'].isin(OT), 'testament'] = 'OT'\n",
    "bible.loc[bible['book_name'].isin(NT), 'testament'] = 'NT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new dataframe with bible with each row as a chapter. These rows will be our documents.\n",
    "bible_chapters = pd.DataFrame(columns = bible.columns)\n",
    "i = 0\n",
    "books = bible.book_name.unique()\n",
    "for book in books:\n",
    "    chapters = bible[bible['book_name'] == book].chapter.unique()\n",
    "    for chapter in chapters:\n",
    "        book_chapter = book + ' ' + str(chapter)\n",
    "        text = bible[(bible['book_name'] == book) & (bible['chapter'] == chapter)].text\n",
    "        if book in OT:\n",
    "            testament = 'OT'\n",
    "        elif book in NT:\n",
    "            testament = 'NT'\n",
    "        bible_chapters.loc[i, ['book_name','chapter','book_chapter','text','testament']] = [book, chapter, book_chapter, ' '.join(text), testament]\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_name</th>\n",
       "      <th>chapter</th>\n",
       "      <th>book_chapter</th>\n",
       "      <th>text</th>\n",
       "      <th>testament</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>1</td>\n",
       "      <td>Genesis 1</td>\n",
       "      <td>In the beginning God created the heavens and t...</td>\n",
       "      <td>OT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>2</td>\n",
       "      <td>Genesis 2</td>\n",
       "      <td>And the heavens and the earth were finished, a...</td>\n",
       "      <td>OT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>3</td>\n",
       "      <td>Genesis 3</td>\n",
       "      <td>Now the serpent was more subtle than any beast...</td>\n",
       "      <td>OT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>4</td>\n",
       "      <td>Genesis 4</td>\n",
       "      <td>And the man knew Eve his wife; and she conceiv...</td>\n",
       "      <td>OT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>5</td>\n",
       "      <td>Genesis 5</td>\n",
       "      <td>This is the book of the generations of Adam. I...</td>\n",
       "      <td>OT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  book_name chapter book_chapter  \\\n",
       "0   Genesis       1    Genesis 1   \n",
       "1   Genesis       2    Genesis 2   \n",
       "2   Genesis       3    Genesis 3   \n",
       "3   Genesis       4    Genesis 4   \n",
       "4   Genesis       5    Genesis 5   \n",
       "\n",
       "                                                text testament  \n",
       "0  In the beginning God created the heavens and t...        OT  \n",
       "1  And the heavens and the earth were finished, a...        OT  \n",
       "2  Now the serpent was more subtle than any beast...        OT  \n",
       "3  And the man knew Eve his wife; and she conceiv...        OT  \n",
       "4  This is the book of the generations of Adam. I...        OT  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_chapters = bible_chapters[['book_name','chapter','book_chapter','text','testament']]\n",
    "bible_chapters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# Utility Functions for word tokenizing document for BOW analysis\n",
    "def bag_of_bags(document_list):\n",
    "    \n",
    "    '''Creating bags of differents spacy token types. Outputs 5 different\n",
    "    bags. Utilize the function as follows:\n",
    "    \n",
    "    allwords, allents, allnps, alltags, alltokens = bag_of_bags(document_list)\n",
    "    '''\n",
    "    \n",
    "    #Intializing Text Parser\n",
    "    from collections import Counter\n",
    "    nlp = spacy.load('en')\n",
    "\n",
    "    #Parsing the Text\n",
    "    text = ' '.join(document_list)\n",
    "    text = nlp(text)\n",
    "        \n",
    "    # Words\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    allwords = [item[0] for item in Counter(allwords).most_common(1000)]\n",
    "    \n",
    "    # Entities\n",
    "    allents = [str(ent)+' ENT' for ent in text.ents]\n",
    "    allents = [item[0] for item in Counter(allents).most_common(250)]\n",
    "\n",
    "    # Noun Phrases\n",
    "    allnps = [str(np)+' NPHR' for np in text.noun_chunks]\n",
    "    allnps = [item[0] for item in Counter(allnps).most_common(500)]\n",
    "    \n",
    "    # Words with POS Tags\n",
    "    alltags = [token.lemma_+' '+token.tag_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    alltags = [item[0] for item in Counter(alltags).most_common(1000)]\n",
    "    \n",
    "    # combining all token categories into one list\n",
    "    alltokens = allwords + allents + allnps + alltags\n",
    "    \n",
    "    return allwords, allents, allnps, alltags, alltokens\n",
    "    \n",
    "def bob_features(document_list):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=alltokens)\n",
    "    df['document_text'] = document_list\n",
    "    df.loc[:, alltokens] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of tokens in each sentence.\n",
    "    for i, document in enumerate(df['document_text']):\n",
    "        \n",
    "        document = nlp(document)\n",
    "        \n",
    "        words = [token.lemma_\n",
    "                 for token in document\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in allwords\n",
    "                 )]\n",
    "        \n",
    "        ents = [str(ent)+' ENT'\n",
    "                 for ent in document.ents\n",
    "                 if str(ent) + ' ' in allents\n",
    "                 ]\n",
    "        \n",
    "        nps = [str(np)+' NPHR'\n",
    "                 for np in document.noun_chunks\n",
    "                 if str(np)+' NPHR' in allnps\n",
    "                 ]\n",
    "        \n",
    "        tags = [token.lemma_+' '+token.tag_\n",
    "                 for token in document\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and (token.lemma_+' '+token.tag_) in alltags\n",
    "                 )]\n",
    "        \n",
    "        tokens = words + ents + nps + tags\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for token in tokens:\n",
    "            df.loc[i, token] += 1\n",
    "            \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 10 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Utility function for word tokenizing documents for tf-idf preparation\n",
    "def document_tokenizer(raw_documents_list):\n",
    "     \n",
    "    '''Takes a list of documents and returns corresponding list (same shape) of lemmatized tokens,\n",
    "     entities, noun_phrases, and POS_tagged words all returned as strings. Each token has spaces replaced\n",
    "     with underscores _ so that tfidfvectorizor can process each token as a single word.\n",
    "     \n",
    "     raw_documents_list should be cleaned of any text that spacy can't process.'''\n",
    "\n",
    "    #Initialize Output List of Document Tokens, and SpaCy NLP Preprocessor\n",
    "    tokenized_documents = []\n",
    "    \n",
    "    #individually spacy process the documents in the original documents list\n",
    "    documents = [ nlp(document) for document in raw_documents_list ]\n",
    "    \n",
    "    #tokenizing text by token category\n",
    "    for text in documents:\n",
    "    \n",
    "        allwords = [token.lemma_\n",
    "                    for token in text\n",
    "                    if not token.is_punct\n",
    "                    and not token.is_stop]\n",
    "\n",
    "        allents = []\n",
    "        for ent in text.ents:\n",
    "            string_ent = str(ent).strip('[]').strip().replace(' ','_') #Spaces in token replaced with underscore #also remove extraneous brackets as last second tweak\n",
    "            allents.append(string_ent + '_ENTITY') #label as entity, differentiate from words\n",
    "\n",
    "        allnps = []\n",
    "        for np in text.noun_chunks:\n",
    "            string_np = str(np).strip('[]').strip().replace(' ','_') #Spaces in token replaced with underscore #also remove extraneous brackets as last second tweak\n",
    "            allnps.append(string_np + '_NPHRASE') #label as NPHRASE, differentiate from words\n",
    "\n",
    "        alltags = [token.lemma_ + '_' + token.tag_ #Spaces in token replaced with underscore\n",
    "                    for token in text\n",
    "                    if not token.is_punct\n",
    "                    and not token.is_stop]\n",
    "        \n",
    "        #aggregating all document token categories into one string per document and appending list of strings\n",
    "        #to the final tokenized document list\n",
    "        doc_tok_cats = [allwords, allents, allnps, alltags] \n",
    "        doc_tokens = [' '.join(cat) for cat in doc_tok_cats]\n",
    "        tokenized_documents.append(' '.join(doc_tokens)) \n",
    "        \n",
    "    return tokenized_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters_clean = []\n",
    "for chapter in bible_chapters.text:\n",
    "    chapters_clean.append(text_cleaner(chapter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4117953"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the length.\n",
    "len(' '.join(chapters_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words (Bag of bags) Features and Modeling to predict which testament a chapter is from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Splitting the data into training and test sets. The sizes of each set can not exceed\n",
    "# 1 million characters in text due to computing limitations, although this number can be changed.\n",
    "bible_df = bible_chapters.copy()\n",
    "bible_df['text'] = chapters_clean\n",
    "train_set, test_set = train_test_split(bible_df, train_size = 0.2, random_state = 5)\n",
    "test_set = resample(test_set, n_samples = len(text_train), random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stitching the bags together; creating these bags is similar to fitting the model,\n",
    "# since it requires feature extraction of a training set.\n",
    "allwords, allents, allnps, alltags, alltokens = bag_of_bags(train_set.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 10\n",
      "Processing row 20\n",
      "Processing row 30\n",
      "Processing row 40\n",
      "Processing row 50\n",
      "Processing row 60\n",
      "Processing row 70\n",
      "Processing row 80\n",
      "Processing row 90\n",
      "Processing row 100\n",
      "Processing row 110\n",
      "Processing row 120\n",
      "Processing row 130\n",
      "Processing row 140\n",
      "Processing row 150\n",
      "Processing row 160\n",
      "Processing row 170\n",
      "Processing row 180\n",
      "Processing row 190\n",
      "Processing row 200\n",
      "Processing row 210\n",
      "Processing row 220\n",
      "Processing row 230\n"
     ]
    }
   ],
   "source": [
    "train_counts = bob_features(list(train_set.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 10\n",
      "Processing row 20\n",
      "Processing row 30\n",
      "Processing row 40\n",
      "Processing row 50\n",
      "Processing row 60\n",
      "Processing row 70\n",
      "Processing row 80\n",
      "Processing row 90\n",
      "Processing row 100\n",
      "Processing row 110\n",
      "Processing row 120\n",
      "Processing row 130\n",
      "Processing row 140\n",
      "Processing row 150\n",
      "Processing row 160\n",
      "Processing row 170\n",
      "Processing row 180\n",
      "Processing row 190\n",
      "Processing row 200\n",
      "Processing row 210\n",
      "Processing row 220\n",
      "Processing row 230\n"
     ]
    }
   ],
   "source": [
    "test_counts = bob_features(list(test_set.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding \n",
    "train_counts['testament'] = train_set['testament']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 42,   1],\n",
       "       [  2, 192]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9873417721518988"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X_train = train_counts.loc[:, ~(train_counts.columns).isin(['document_text','testament'])]\n",
    "Y_train = train_set['testament']\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train,Y_train)\n",
    "predict = clf.predict(test_counts.loc[:, ~(test_counts.columns).isin(['document_text'])])\n",
    "conmat = confusion_matrix(test_set['testament'], predict)\n",
    "display(conmat)\n",
    "clf.score(test_counts.loc[:, ~(test_counts.columns).isin(['document_text'])], test_set['testament'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model turned out great. Prediction accuracy when rounded is 99%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Model using TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating column to tokenize each chapter \n",
    "nlp = spacy.load('en')\n",
    "bible_df['token_text'] = document_tokenizer(bible_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 10822\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Creating training and test sets for TF-IDF\n",
    "train_set, test_set = train_test_split(bible_df, train_size = 0.2, random_state = 6)\n",
    "#test_set = resample(test_set, n_samples = len(text_train), random_state = 11)\n",
    "\n",
    "X_train = train_set['token_text']\n",
    "Y_train = train_set['testament']\n",
    "X_test = test_set['token_text']\n",
    "Y_test = test_set['testament']\n",
    "\n",
    "#TF-IDF Transforming the text using X_train training set\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=False, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "print(\"Number of features: %d\" % X_train_tfidf.get_shape()[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[136,  60],\n",
       "       [  0, 756]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9369747899159664"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a predictive model\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_tfidf, Y_train)\n",
    "predict = clf.predict(X_test_tfidf)\n",
    "conmat = confusion_matrix(Y_test, predict)\n",
    "display(conmat)\n",
    "clf.score(X_test_tfidf, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.94957983, 0.98319328, 0.94957983, 0.97478992, 0.88185654])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=False, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "clf = LogisticRegression()\n",
    "\n",
    "tfidf_clf_model = Pipeline(steps=[('tfidf', vectorizer),('lreg classifier', clf)])\n",
    "cross_val_score(tfidf_clf_model, bible_df['token_text'], bible_df['testament'], cv = 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross validation scores all look great besides the last fold, which creates a 10% spread between itself and the highest score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
